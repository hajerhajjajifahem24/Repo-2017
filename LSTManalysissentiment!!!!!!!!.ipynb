{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTManalysissentiment!!!!!!!!.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQrOAeCw7//XEj7yPFG4Qv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hajerhajjajifahem24/Repo-2017/blob/master/LSTManalysissentiment!!!!!!!!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZaQbPT3Ecvb"
      },
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import re\n",
        "import string\n",
        "import nltk \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words  = nltk.corpus.stopwords.words('english')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYx5aZ2zEuqK"
      },
      "source": [
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDtaZWYSFJDH"
      },
      "source": [
        "data = pd.read_csv('googleplaystore_user_reviews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSDwMLbbFqmI"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2tLRCDQKlB3"
      },
      "source": [
        "data=(data[[\"Translated_Review\",\"Sentiment\"]]).dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8quxeoAwO3qG"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwwNtq-kF5rZ"
      },
      "source": [
        "dataN = data[data.Sentiment == \"Neutral\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x82OkNmAGA8a"
      },
      "source": [
        "dataN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnWe3HAm162K"
      },
      "source": [
        "datap = data[data.Sentiment == \"Positive\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCh3qUSu2BNh"
      },
      "source": [
        "datap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLcUEILuvNIM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReFGlCmbvOzg"
      },
      "source": [
        "datapne = data[data.Sentiment == \"Negative\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uYgeqGlvcyn"
      },
      "source": [
        "datapne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eGa0vtslrP4"
      },
      "source": [
        "**data processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zes_mzqKwwb"
      },
      "source": [
        "def preprocess_text_text(text):\n",
        "    text.lower()\n",
        "   \n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    text=re.sub(r'[0-9]+', '', text)\n",
        "    \n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    text_tokens = word_tokenize(text)\n",
        "    filtered_words = [w for w in text_tokens if not w in stop_words]\n",
        "    \n",
        "    ps = PorterStemmer()\n",
        "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
        "    \n",
        "    return \" \".join(filtered_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C5fvFFILVLJ"
      },
      "source": [
        "data[\"Translated_Review\"]=data.Translated_Review.apply(preprocess_text_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM7JrJ8QWU5U"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOmjoGDpX4Pl"
      },
      "source": [
        "data['Translated_Review'] = data['Translated_Review'].apply(lambda x: x.lower())\n",
        "# removing special chars\n",
        "data['Translated_Review'] = data['Translated_Review'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EupEJI11lyjg"
      },
      "source": [
        "**`tokenizer data**\n",
        "\n",
        "---\n",
        "\n",
        "**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5eHhdQ6YILl"
      },
      "source": [
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['Translated_Review'].values)\n",
        "X = tokenizer.texts_to_sequences(data['Translated_Review'].values)\n",
        "X = pad_sequences(X)\n",
        "\n",
        "Y = pd.get_dummies(data['Sentiment']).values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXVOL4BQGDzr"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSTYEMz7GDef"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkPOf_vVGC98"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNy0QPbfxQJC"
      },
      "source": [
        "X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4mVzFyPl9MA"
      },
      "source": [
        "**model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUENxK9gZxlh"
      },
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5mors_GmBH7"
      },
      "source": [
        "**fit model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jg8EuMEZ9ay"
      },
      "source": [
        "batch_size = 128\n",
        "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upW5UTNdmKii"
      },
      "source": [
        "**matrix confusion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWhvbp95j_0s"
      },
      "source": [
        "Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
        "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
        "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
        "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
        "print(classification_report(df_test.true, df_test.pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orohf5uDntIu"
      },
      "source": [
        "predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfXhGqpLk2zn"
      },
      "source": [
        "testtext = ['very bad']\n",
        "testtext= tokenizer.texts_to_sequences(testtext)\n",
        "testtext = pad_sequences(testtext, maxlen=29, dtype='int32', value=0)\n",
        "print(testtext)\n",
        "sentiment = model.predict(testtext,batch_size=1,verbose = 2)[0]\n",
        "if(np.argmax(sentiment) == 0):\n",
        "    print(\"negative\")\n",
        "elif (np.argmax(sentiment) == 1):\n",
        "     print(\"positive\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhhplfbdnwq2"
      },
      "source": [
        "upload file reviwes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jndN0YAEt4Sd"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hBpxOYbNi8V"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fokLIF8aAT9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcNmK031uRL4"
      },
      "source": [
        "test = pd.read_csv('reviewsapp.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1MAVLIWvHdc"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF5MoSRKn3KV"
      },
      "source": [
        "**data processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls56LokqudW5"
      },
      "source": [
        "test['content'] = test['content'].apply(lambda x: x.lower())\n",
        "# removing special chars\n",
        "test['content'] = test['content'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HevEwUrtOCvd"
      },
      "source": [
        "test['content']=test.content.apply(preprocess_text_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlHGWNSLu0J1"
      },
      "source": [
        "test['content'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NpSL57i2M31"
      },
      "source": [
        "work count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcrsz9XLMYdm"
      },
      "source": [
        "test['content'][0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1orHJzwoH-_"
      },
      "source": [
        "**tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noHEKWpfvhP8"
      },
      "source": [
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(test['content'].values)\n",
        "X1 = tokenizer.texts_to_sequences(test['content'].values)\n",
        "X1 = pad_sequences(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFnpwhW3wRub"
      },
      "source": [
        "len(X1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO0Xg0R9wY-4"
      },
      "source": [
        "\n",
        "X1.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Us0eD-4ybci"
      },
      "source": [
        "l=len(X1[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne7OvigGoPcV"
      },
      "source": [
        "Predict the reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ9l6xVXvp-Y"
      },
      "source": [
        "app_reviews=[]\n",
        "sentiment=[]\n",
        "for i in range (l) :\n",
        "  sentiment= model.predict(X[i] ,batch_size=1,verbose = 2)[0]\n",
        "  if(np.argmax(sentiment) == 0):\n",
        "     senti=\"negative\"\n",
        "      #print(\"negative\")\n",
        "      \n",
        "  elif (np.argmax(sentiment) == 1):\n",
        "        senti=\"positive\"\n",
        "      #print(\"positive\")\n",
        "\n",
        "  app_reviews.append(senti)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqlQDYw7hP84"
      },
      "source": [
        "app_reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_9Q2WthSX6P"
      },
      "source": [
        "app_reviews=[]\n",
        "sentiment = model.predict(X[2] ,batch_size=1,verbose = 2)[0]\n",
        "sentiment1 = model.predict(X[1] ,batch_size=1,verbose = 2)[0]\n",
        "if(np.argmax(sentiment) == 0):\n",
        "     senti=\"negative\"\n",
        "      #print(\"negative\")\n",
        "elif (np.argmax(sentiment) == 1):\n",
        "        senti=\"positive\"\n",
        "      #print(\"positive\")\"\n",
        "if(np.argmax(sentiment1) == 0):\n",
        "     senti1=\"negative\"\n",
        "      #print(\"negative\")\n",
        "elif (np.argmax(sentiment1) == 1):\n",
        "        senti1=\"positive\"\n",
        "      #print(\"positive\")\"     \n",
        "app_reviews.append(senti)    \n",
        "app_reviews.append(senti1)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6nJC-B5zOkN"
      },
      "source": [
        "app_reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXYeYiSmodsE"
      },
      "source": [
        "**convert the result to dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHdHbF7Z36QT"
      },
      "source": [
        "\n",
        "d = {'sentiment':app_reviews}\n",
        "app_reviews_data = pd.DataFrame(data=d)\n",
        "app_reviews_data.to_csv('reviewsapp.csv', index=None, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdETByPqpGi6"
      },
      "source": [
        "files.download(\"reviewsapp.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6QBZ2aDo9uY"
      },
      "source": [
        "**[bluid corpus of the word ](https:// [link text](https://))**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SuYulGOpWzi"
      },
      "source": [
        "data prcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRKldGWVoA4p"
      },
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "corpus=[]\n",
        "for i in range(0,10000):\n",
        "    review = re.sub('[^a-zA-Z]', ' ',test['content'][i])\n",
        "    review = re.sub('[/(){}\\[\\]\\|@!,;]', ' ',test['content'][i])\n",
        "    #review = re.sub('[^0-9#+_♥️]', ' ',test['content'][i])#Remove bad symbols\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    stemmer = PorterStemmer()\n",
        "    review = [stemmer.stem(token) for token in review if token not in STOPWORDS]\n",
        "    review=' '.join(review)\n",
        "    corpus.append(review)\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKSHDVVipbEi"
      },
      "source": [
        "bag of work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeCKz1Jj4FNj"
      },
      "source": [
        "words = []\n",
        "Sentiment_Polarity=[]\n",
        "for i in range(0,len(corpus)):\n",
        "    words = words + (re.findall(r'\\w+', corpus[i]))# words cantain all the words in the dataset\n",
        "    Sentiment_Polarity.append\n",
        "words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG9o8gOZpf3A"
      },
      "source": [
        ""
      ]
    }
  ]
}